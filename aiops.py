# -*- coding: utf-8 -*-
"""aiops.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bE5q6gNzw9bKyTlcaUSa5ZJqPrhO-pSj
"""

import pandas as pd
import numpy as np
import networkx as nx
from datetime import datetime, timedelta
from collections import Counter

np.random.seed(42)

#Defining pods and nodes
pods = [f"pod-{i}" for i in range(1, 4)]
nodes = [f"node-{i}" for i in (1, 2)]

#Time series..clocking 12 points every 5 min from 16th May
start = datetime(2025, 5, 16, 9, 0)
times = [start + timedelta(minutes=5 * i) for i in range(12)]

#Metrics DataFrame
metrics_records = []
for pod in pods:
    for t in times:
        metrics_records.append({
            "timestamp": t,
            "resource": pod,
            "cpu_usage": np.random.normal(50, 10),
            "memory_usage": np.random.normal(500, 50),
            "disk_usage": np.random.normal(200, 20)
        })
metrics_df = pd.DataFrame(metrics_records)

#Alerts DataFrame
alerts_records = []
alert_types = ["HighCPU", "HighMemory", "DiskFull"]
for pod in pods:
    t = np.random.choice(times)
    alerts_records.append({
        "timestamp": t,
        "alert_type": np.random.choice(alert_types),
        "resource": pod,
        "severity": np.random.choice(["critical", "warning"])
    })
alerts_df = pd.DataFrame(alerts_records)

#Logs DataFrame
logs_records = []
log_messages = ["OOMKilled", "Heartbeat failed", "Started container", "Back-off restarting"]
for pod in pods:
    for msg in log_messages[:2]:  # two logs per pod
        t = np.random.choice(times)
        logs_records.append({
            "timestamp": t,
            "pod": pod,
            "message": msg
        })
logs_df = pd.DataFrame(logs_records)

#Display synthetic datasets - metrics
metrics_df.head()

#Display synthetic datasets - alerts
alerts_df.head()

#Display synthetic datasets - logs
logs_df.head()

G = nx.Graph()

#Add Pod and Node nodes
for pod in pods:
    G.add_node(pod, type="Pod")
for node in nodes:
    G.add_node(node, type="Node")

for pod in pods:
    G.add_edge(pod, np.random.choice(nodes), type="RUNS_ON")

for idx, row in metrics_df.iterrows():
    m_id = f"metric-{idx}"
    G.add_node(m_id, type="Metric", timestamp=row["timestamp"],
               values={"cpu": row["cpu_usage"], "mem": row["memory_usage"], "disk": row["disk_usage"]})
    G.add_edge(row["resource"], m_id, type="HAS_METRIC")

for idx, row in alerts_df.iterrows():
    a_id = f"alert-{idx}"
    G.add_node(a_id, type="Alert", timestamp=row["timestamp"],
               alert_type=row["alert_type"], severity=row["severity"])
    G.add_edge(row["resource"], a_id, type="RAISED_ALERT")

for idx, row in logs_df.iterrows():
    l_id = f"log-{idx}"
    G.add_node(l_id, type="LogEvent", timestamp=row["timestamp"], message=row["message"])
    G.add_edge(row["pod"], l_id, type="EMITS_LOG")

#correlation
for l_id, l_data in [(n, G.nodes[n]) for n in G.nodes if G.nodes[n]["type"] == "LogEvent"]:
    for a_id, a_data in [(n, G.nodes[n]) for n in G.nodes if G.nodes[n]["type"] == "Alert"]:
        time_diff = abs((l_data["timestamp"] - a_data["timestamp"]).total_seconds())
        if time_diff <= 300:
            G.add_edge(l_id, a_id, type="CORRELATED_WITH")

#extract on demand sub graph
issue_pod = "pod-1"
sub = nx.ego_graph(G, issue_pod, radius=2)

#Subgraph
type_counts = Counter(nx.get_node_attributes(sub, 'type').values())
print(f"Subgraph centered on '{issue_pod}':")
print(" Node counts by type:", dict(type_counts))
print(" Sample nodes:", list(sub.nodes)[:10])
print(" Sample edges:", list(sub.edges(data=True))[:10])

import matplotlib.pyplot as plt

#hop distance from the node where there is issue
distances = nx.single_source_shortest_path_length(sub, issue_pod)

#Build and display DF - nodes, their types & hop distances
node_types = nx.get_node_attributes(sub, 'type')
dist_df = pd.DataFrame([
    {"node": node, "type": node_types.get(node, "Unknown"), "hops_from_issue": distances[node]}
    for node in sub.nodes()
]).sort_values("hops_from_issue")
print("Subgraph Nodes & Hop Distances", dist_df.head())

#Visualize the subgraph with hop distances in the labels
labels = {n: f"{n}\n({distances[n]})" for n in sub.nodes()}
pos = nx.spring_layout(sub, seed=42)

plt.figure(figsize=(10, 8))
nx.draw(sub, pos, labels=labels, node_size=300, font_size=8)
plt.title(f"Subgraph around '{issue_pod}' with Hop Distances")
plt.axis('off')
plt.show()

def build_subgraph_in_memory(pod_id: str, radius: int = 2):

    # Extract the ego graph (nodes within 'radius' hops of pod_id)
    sub = nx.ego_graph(G, pod_id, radius=radius)

    # Prepare node list with attributes
    nodes = [(n, dict(G.nodes[n])) for n in sub.nodes()]

    # Prepare edge list with attributes
    edges = [(u, v, dict(sub.edges[u, v])) for u, v in sub.edges()]

    return nodes, edges

nodes, edges = build_subgraph_in_memory("pod-1", radius=2)
print(f"Subgraph has {len(nodes)} nodes and {len(edges)} edges.")

!pip install --upgrade \
  langchain[openai,agents,tools] \
  langchain-community \
  openai \
  neo4j \
  networkx \
  sentence-transformers \
  faiss-cpu

from langchain import LLMChain
from langchain.llms import OpenAI
from langchain.agents import Tool, initialize_agent, AgentType

import os
os.environ["OPENAI_API_KEY"] = ""

from langchain.chat_models import ChatOpenAI

# either via env var
import os
os.environ["OPENAI_API_KEY"] = ""

chat = ChatOpenAI(
    model_name="gpt-4o-mini",
    temperature=0.2,
)

llm = OpenAI(
    model_name="gpt-4.1-mini",
    temperature=0.5,
)

#LangChain Tools

#Metric Retriever
def get_metrics(pod_id, since):
    # filter metrics_df defined earlier
    df = metrics_df[(metrics_df["resource"]==pod_id) & (metrics_df["timestamp"]>=since)]
    return df.to_dict(orient="records")

metric_tool = Tool(
    name="get_metrics",
    func=get_metrics,
    description="Fetch recent CPU/memory/disk metrics for a pod."
)

#Alert Retriever
def get_alerts(pod_id, since):
    df = alerts_df[(alerts_df["resource"]==pod_id) & (alerts_df["timestamp"]>=since)]
    return df.to_dict(orient="records")

alert_tool = Tool(
    name="get_alerts",
    func=get_alerts,
    description="Fetch recent alerts for a pod."
)

#Log Retriever
def get_logs(pod_id, since):
    df = logs_df[(logs_df["pod"]==pod_id) & (logs_df["timestamp"]>=since)]
    return df.to_dict(orient="records")

log_tool = Tool(
    name="get_logs",
    func=get_logs,
    description="Fetch recent log events for a pod."
)

#Graph Query Tool
def query_subgraph(pod_id):
    # reuse the build_subgraph logic from above
    nodes, edges = build_subgraph_in_memory(pod_id, radius=2)
    return {"nodes": nodes, "edges": edges}

graph_tool = Tool(
    name="query_graph",
    func=query_subgraph,
    description="Return the 2-hop subgraph around a pod for RCA."
)

#Initialize the Agent
tools = [metric_tool, alert_tool, log_tool, graph_tool]
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# then pass `chat` into your agent
from langchain.agents import initialize_agent, Tool, AgentType

agent = initialize_agent(
    [metric_tool, alert_tool, log_tool, graph_tool],
    chat,
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)

!pip install openai==0.28.0

from typing import List, Dict
from langchain.tools import tool

@tool("get_alerts", return_direct=False, description="Fetch recent alerts for a given pod and namespace.")
def get_alerts(pod_id: str, namespace: str) -> List[Dict]:
    # your real logic here; for demo we’ll filter alerts_df
    df = alerts_df[
        (alerts_df["resource"] == pod_id) &
        (alerts_df.get("namespace", "default") == namespace)
    ]
    return df.to_dict(orient="records")

from pydantic import BaseModel, Field
from langchain.tools import StructuredTool

class GetAlertsArgs(BaseModel):
    pod_id: str = Field(..., description="The ID of the Kubernetes pod")
    namespace: str = Field(..., description="The Kubernetes namespace")

def _get_alerts(pod_id: str, namespace: str):
    df = alerts_df[
        (alerts_df["resource"] == pod_id) &
        (alerts_df.get("namespace", "default") == namespace)
    ]
    return df.to_dict(orient="records")

get_alerts_tool = StructuredTool.from_function(
    func=_get_alerts,
    args_schema=GetAlertsArgs,
    name="get_alerts",
    description="Fetch recent alerts for a given pod and namespace."
)

from langchain.agents import initialize_agent, AgentType

tools = [metric_tool, get_alerts_tool, log_tool, graph_tool]
# agent = initialize_agent(
#     tools,
#     chat,  # your ChatOpenAI instance
#     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
#     verbose=True
# )

agent = initialize_agent(
    tools,
    chat,
    agent=AgentType.OPENAI_FUNCTIONS,  # ← use function‐calling agent
    verbose=True,
)

prompt = (
    "Pod 'pod-1' in namespace 'default' isn’t running since 2025-05-16T09:45. "
    "Use get_metrics, get_alerts, get_logs, and query_graph to find the root cause "
    "and suggest remediation steps, showing your reasoning."
)
response = agent.run(prompt)
print(response)

np.random.seed(42)
# Define pods and nodes
pods = [f"pod-{i}" for i in range(1, 4)]
nodes = [f"node-{i}" for i in (1, 2)]
# Time window: 12 points every 5 minutes starting May 16, 2025 09:00
start = datetime(2025, 5, 16, 9, 0)
times = [start + timedelta(minutes=5 * i) for i in range(12)]

# Metrics DataFrame
metrics_records = []
for pod in pods:
    for t in times:
        metrics_records.append({
            "timestamp": t,
            "resource": pod,
            "cpu_usage": np.random.normal(50, 10),
            "memory_usage": np.random.normal(500, 50),
            "disk_usage": np.random.normal(200, 20)
        })
metrics_df = pd.DataFrame(metrics_records)

# Alerts DataFrame
alerts_records = []
alert_types = ["HighCPU", "HighMemory", "DiskFull"]
for pod in pods:
    t = np.random.choice(times)
    alerts_records.append({
        "timestamp": t,
        "alert_type": np.random.choice(alert_types),
        "resource": pod,
        "severity": np.random.choice(["critical", "warning"])
    })
alerts_df = pd.DataFrame(alerts_records)

logs_records = []
log_messages = ["OOMKilled", "Heartbeat failed", "Started container", "Back-off restarting"]
for pod in pods:
    for msg in log_messages[:2]:
        t = np.random.choice(times)
        logs_records.append({
            "timestamp": t,
            "pod": pod,
            "message": msg
        })
logs_df = pd.DataFrame(logs_records)

G = nx.Graph()
# Add Pod and Node nodes
for pod in pods:
    G.add_node(pod, type="Pod")
for node in nodes:
    G.add_node(node, type="Node")
# Connect pods to nodes
for pod in pods:
    G.add_edge(pod, np.random.choice(nodes), type="RUNS_ON")
# Add metric event nodes
for idx, row in metrics_df.iterrows():
    m_id = f"metric-{idx}"
    G.add_node(m_id, type="Metric", timestamp=row["timestamp"],
               cpu=row["cpu_usage"], memory=row["memory_usage"], disk=row["disk_usage"])
    G.add_edge(row["resource"], m_id, type="HAS_METRIC")
# Add alert event nodes
for idx, row in alerts_df.iterrows():
    a_id = f"alert-{idx}"
    G.add_node(a_id, type="Alert", timestamp=row["timestamp"],
               alert_type=row["alert_type"], severity=row["severity"])
    G.add_edge(row["resource"], a_id, type="RAISED_ALERT")
# Add log event nodes
for idx, row in logs_df.iterrows():
    l_id = f"log-{idx}"
    G.add_node(l_id, type="LogEvent", timestamp=row["timestamp"], message=row["message"])
    G.add_edge(row["pod"], l_id, type="EMITS_LOG")
# Correlate logs↔alerts within 5 minutes
for l in [n for n, d in G.nodes(data=True) if d["type"]=="LogEvent"]:
    for a in [n for n, d in G.nodes(data=True) if d["type"]=="Alert"]:
        dt = abs((G.nodes[l]["timestamp"] - G.nodes[a]["timestamp"]).total_seconds())
        if dt <= 300:
            G.add_edge(l, a, type="CORRELATED_WITH")

def build_subgraph_in_memory(pod_id: str, radius: int = 2):
    sub = nx.ego_graph(G, pod_id, radius=radius)
    nodes = [(n, dict(sub.nodes[n])) for n in sub.nodes()]
    edges = [(u, v, dict(sub.edges[u, v])) for u, v in sub.edges()]
    return nodes, edges

@tool(
    "get_metrics",
    return_direct=True,
    description="Fetch metrics for a pod since an ISO timestamp."
)
def get_metrics(pod_id: str, since: str):
    since_ts = pd.to_datetime(since)
    df = metrics_df[
        (metrics_df["resource"] == pod_id) &
        (metrics_df["timestamp"] >= since_ts)
    ]
    return df.to_dict(orient="records")

@tool(
    "get_alerts",
    return_direct=True,
    description="Fetch alerts for a pod since an ISO timestamp."
)
def get_alerts(pod_id: str, since: str):
    since_ts = pd.to_datetime(since)
    df = alerts_df[
        (alerts_df["resource"] == pod_id) &
        (alerts_df["timestamp"] >= since_ts)
    ]
    return df.to_dict(orient="records")

@tool(
    "get_logs",
    return_direct=True,
    description="Fetch log events for a pod since an ISO timestamp."
)
def get_logs(pod_id: str, since: str):
    since_ts = pd.to_datetime(since)
    df = logs_df[
        (logs_df["pod"] == pod_id) &
        (logs_df["timestamp"] >= since_ts)
    ]
    return df.to_dict(orient="records")

@tool(
    "query_graph",
    return_direct=True,
    description="Return the 2-hop subgraph around a pod."
)
def query_graph(pod_id: str, radius: int = 2):
    nodes, edges = build_subgraph_in_memory(pod_id, radius)
    return {"nodes": nodes, "edges": edges}

tools = [get_metrics, get_alerts, get_logs, query_graph]

agent = initialize_agent(
    tools,
    chat,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True
)

prompt = (
    "Pod 'pod-1' in namespace 'default' isn’t running since 2025-05-16T09:45:00. "
    "Use get_metrics, get_alerts, get_logs, and query_graph to identify the root cause "
    "and suggest remediation steps, including your reasoning."
)

response = agent.run(prompt)
print(response)

SYSTEM_PROMPT = """
You are a Root Cause Analysis Assistant.
Whenever you receive a problem description, you MUST:
  1. Plan your approach out loud (step-by-step).
  2. Use the available tools (get_metrics, get_alerts, get_logs, query_graph) to gather evidence.
  3. After each tool call, reflect on what the data tells you.
  4. Once you have enough evidence, clearly state:
     a. The most likely root cause.
     b. At least two concrete remediation steps.
Always show your chain of thought before the final answer.
"""

USER_PROMPT = """
Problem: Pod '{pod_id}' in namespace '{namespace}' isn’t running since {since}.

Follow the plan:
- Tool calls must be explicit.
- Reflections must cite what each tool returned.
- Conclude with **Root Cause** and **Remediation Steps**.

Answer in one message.
"""

agent = initialize_agent(
    tools,
    chat,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True,
    prefix=SYSTEM_PROMPT,
    suffix=USER_PROMPT,
    input_variables=["pod_id", "namespace", "since"],
)

response = agent.run({
    "pod_id": "pod-1",
    "namespace": "default",
    "since": "2025-05-16T09:45:00"
})
print(response)

# script.py

# 0. Install dependencies (run in your shell):
#    pip install langchain[openai,agents,tools] openai networkx pandas numpy

# import os
# from datetime import datetime, timedelta
# import numpy as np
# import pandas as pd
# import networkx as nx

# from langchain.tools import tool
# from langchain.chat_models import ChatOpenAI
# from langchain.agents import initialize_agent, AgentType

# 1. Synthetic Data Generation
np.random.seed(42)
pods = [f"pod-{i}" for i in range(1, 4)]
nodes = [f"node-{i}" for i in (1, 2)]
start = datetime(2025, 5, 16, 9, 0)
times = [start + timedelta(minutes=5 * i) for i in range(12)]

metrics_records = []
for pod in pods:
    for t in times:
        metrics_records.append({
            "timestamp": t,
            "resource": pod,
            "cpu_usage": np.random.normal(50, 10),
            "memory_usage": np.random.normal(500, 50),
            "disk_usage": np.random.normal(200, 20)
        })
metrics_df = pd.DataFrame(metrics_records)

alerts_records = []
alert_types = ["HighCPU", "HighMemory", "DiskFull"]
for pod in pods:
    t = np.random.choice(times)
    alerts_records.append({
        "timestamp": t,
        "alert_type": np.random.choice(alert_types),
        "resource": pod,
        "severity": np.random.choice(["critical", "warning"])
    })
alerts_df = pd.DataFrame(alerts_records)

logs_records = []
log_msgs = ["OOMKilled", "Heartbeat failed"]
for pod in pods:
    for msg in log_msgs:
        t = np.random.choice(times)
        logs_records.append({
            "timestamp": t,
            "pod": pod,
            "message": msg
        })
logs_df = pd.DataFrame(logs_records)

# 2. Build In-Memory Graph
G = nx.Graph()
for pod in pods:
    G.add_node(pod, type="Pod")
for node in nodes:
    G.add_node(node, type="Node")
for pod in pods:
    G.add_edge(pod, np.random.choice(nodes), type="RUNS_ON")

for idx, row in metrics_df.iterrows():
    mid = f"metric-{idx}"
    G.add_node(mid, type="Metric", timestamp=row["timestamp"],
               cpu=row["cpu_usage"], memory=row["memory_usage"], disk=row["disk_usage"])
    G.add_edge(row["resource"], mid, type="HAS_METRIC")

for idx, row in alerts_df.iterrows():
    aid = f"alert-{idx}"
    G.add_node(aid, type="Alert", timestamp=row["timestamp"],
               alert_type=row["alert_type"], severity=row["severity"])
    G.add_edge(row["resource"], aid, type="RAISED_ALERT")

for idx, row in logs_df.iterrows():
    lid = f"log-{idx}"
    G.add_node(lid, type="LogEvent", timestamp=row["timestamp"], message=row["message"])
    G.add_edge(row["pod"], lid, type="EMITS_LOG")

# correlate logs ↔ alerts within 5 minutes
for lid, ldata in [(n, G.nodes[n]) for n in G if G.nodes[n]["type"]=="LogEvent"]:
    for aid, adata in [(n, G.nodes[n]) for n in G if G.nodes[n]["type"]=="Alert"]:
        if abs((ldata["timestamp"] - adata["timestamp"]).total_seconds()) <= 300:
            G.add_edge(lid, aid, type="CORRELATED_WITH")

# 3. Subgraph Builder
def build_subgraph_in_memory(pod_id: str, radius: int = 2):
    sub = nx.ego_graph(G, pod_id, radius=radius)
    nodes = [(n, dict(sub.nodes[n])) for n in sub.nodes()]
    edges = [(u, v, dict(sub.edges[u, v])) for u, v in sub.edges()]
    return nodes, edges

# 4. LangChain Tools
@tool("get_metrics", return_direct=True, description="Fetch metrics for pod since ISO timestamp")
def get_metrics(pod_id: str, since: str):
    since_ts = pd.to_datetime(since)
    df = metrics_df[
        (metrics_df["resource"]==pod_id) &
        (metrics_df["timestamp"]>=since_ts)
    ]
    return df.to_dict(orient="records")

@tool("get_alerts", return_direct=True, description="Fetch alerts for pod since ISO timestamp")
def get_alerts(pod_id: str, since: str):
    since_ts = pd.to_datetime(since)
    df = alerts_df[
        (alerts_df["resource"]==pod_id) &
        (alerts_df["timestamp"]>=since_ts)
    ]
    return df.to_dict(orient="records")

@tool("get_logs", return_direct=True, description="Fetch logs for pod since ISO timestamp")
def get_logs(pod_id: str, since: str):
    since_ts = pd.to_datetime(since)
    df = logs_df[
        (logs_df["pod"]==pod_id) &
        (logs_df["timestamp"]>=since_ts)
    ]
    return df.to_dict(orient="records")

@tool("query_graph", return_direct=True, description="Return 2-hop subgraph around pod")
def query_graph(pod_id: str, radius: int = 2):
    nodes, edges = build_subgraph_in_memory(pod_id, radius)
    return {"nodes": nodes, "edges": edges}

# 5. Initialize Agent
os.environ["OPENAI_API_KEY"] = ""
chat = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0)
tools = [get_metrics, get_alerts, get_logs, query_graph]
agent = initialize_agent(tools, chat, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)

# 6. Run RCA Prompt
pod_id = "pod-1"
since = "2025-05-16T09:45:00"
prompt = (
    "You are a Root Cause Analysis Assistant. Think step by step, call the tools to gather data, "
    "reflect after each, then conclude with a Root Cause and Remediation Steps.\n"
    f"Problem: Pod '{pod_id}' isn't running since {since}. "
    "Use get_metrics, get_alerts, get_logs, and query_graph."
)
response = agent.run(prompt)
print(response)

# 6. Run RCA Prompt (improved)

pod_id = "pod-1"
since  = "2025-05-16T09:45:00"

prompt = f"""
You are a Kubernetes Root Cause Analysis Assistant.

When given a problem you must:
1) THINK which tool to call first.
2) CALL the tool with exact arguments.
3) OBSERVE the output and REFLECT on what it means.
4) REPEAT until you have enough evidence.
5) CONCLUDE with:
   • Root Cause:
   • Remediation Steps (at least two).

Problem: Pod '{pod_id}' in namespace 'default' isn't running since {since}.

Available tools:
• get_metrics(pod_id, since)
• get_alerts(pod_id, since)
• get_logs(pod_id, since)
• query_graph(pod_id, radius)

Begin now.
"""

print("=== Sending to agent: ===")
print(prompt)
print("=========================")

# this will log each Thought/Action/Observation to your console (because verbose=True)
response = agent.run(prompt)

print("\n=== Agent’s full reasoning and RCA ===\n")
print(response)

!pip install openai pandas networkx numpy

import openai

pod_id = "pod-1"
since  = "2025-05-16T09:45:00"

# assume these exist from your earlier code:
# metrics_df, alerts_df, logs_df, build_subgraph_in_memory()

metrics = metrics_df[
    (metrics_df.resource == pod_id) &
    (metrics_df.timestamp >= pd.to_datetime(since))
].to_dict(orient="records")

alerts = alerts_df[
    (alerts_df.resource == pod_id) &
    (alerts_df.timestamp >= pd.to_datetime(since))
].to_dict(orient="records")

logs = logs_df[
    (logs_df.pod == pod_id) &
    (logs_df.timestamp >= pd.to_datetime(since))
].to_dict(orient="records")

nodes, edges = build_subgraph_in_memory(pod_id, radius=2)

# ─── 2) Construct your chat messages ───
system_msg = {
    "role": "system",
    "content": (
        "You are a Root Cause Analysis Assistant.\n"
        "Think step by step and show your full reasoning. "
        "Then conclude with:\n"
        "1. Root Cause:\n"
        "2. Remediation Steps (at least two).\n"
    )
}

user_msg = {
    "role": "user",
    "content": (
        f"Problem: Pod '{pod_id}' isn't running since {since}.\n\n"
        f"Metrics (since {since}):\n{pd.DataFrame(metrics).to_markdown(index=False)}\n\n"
        f"Alerts:\n{pd.DataFrame(alerts).to_markdown(index=False)}\n\n"
        f"Logs:\n{pd.DataFrame(logs).to_markdown(index=False)}\n\n"
        f"Graph Subgraph:\n"
        f" • Nodes (id,type): {[(n,d['type']) for n,d in nodes]}\n"
        f" • Edges (u→v,type): {[(u,v,d['type']) for u,v,d in edges]}\n\n"
        "Please think out loud, then answer as specified."
    )
}

# ─── 3) Call the OpenAI ChatCompletion API ───
resp = openai.ChatCompletion.create(
    model="gpt-4o-mini",
    messages=[system_msg, user_msg],
    temperature=0.0,
)

# ─── 4) Print out full reasoning + RCA ───
print(resp.choices[0].message.content)

